ОТВЕТЫ НА КОНТРОЛЬНЫЕ ВОПРОСЫ
Практическая работа №10: Профилирование и оптимизация параллельных программ

===================================================================================

ВОПРОС 1: В чём отличие измерения времени выполнения от профилирования?

Измерение времени выполнения это когда мы просто засекаем сколько времени заняла программа или её часть. Ставим таймер в начале, в конце, вычитаем и получаем число. Это как засечь время пробежки секундомером - узнали общее время, но не знаем где именно бежали быстро, а где медленно.

Примеры измерения времени:
- omp_get_wtime() - засекаем начало и конец функции
- std::chrono в C++ - то же самое
- cudaEvent - замеряем время на GPU
Получаем одно число: "программа выполнилась за 5.2 секунды"

Профилирование это когда мы детально анализируем что происходит внутри программы. Профилировщик следит за выполнением и собирает кучу информации:
- Сколько времени заняла каждая функция
- Сколько раз вызывалась каждая функция
- Где программа тормозит (hotspots)
- Сколько времени ушло на ожидание (I/O, синхронизация)
- Использование кеша и памяти
- Загрузка процессора и GPU

Примеры профилировщиков:
- gprof для CPU программ
- Intel VTune - продвинутый профилировщик
- NVIDIA Nsight для CUDA
- Valgrind для анализа памяти

Аналогия:
Измерение времени = узнать что пробежал 10 км за час
Профилирование = получить детальную карту где бежал быстро, где медленно, где останавливался, пульс, темп на каждом километре

Когда что использовать:
- Измерение времени: быстрая оценка, сравнение вариантов, простая отладка
- Профилирование: понять где именно проблемы, найти узкие места, глубокая оптимизация

На практике:
Сначала измеряем время чтобы понять что есть проблема.
Потом профилируем чтобы понять где именно проблема.
Оптимизируем проблемное место.
Снова измеряем чтобы проверить что стало лучше.

===================================================================================

ВОПРОС 2: Какие виды узких мест характерны для CPU, GPU и распределённых программ?

Узкое место - это часть программы которая тормозит всё остальное. Как пробка на дороге - все машины едут медленно из-за одного участка.

CPU УЗКИЕ МЕСТА:

1. Последовательные участки кода
Часть программы которую нельзя распараллелить. Все потоки ждут пока один поток закончит работу.
Пример: чтение конфигурационного файла в начале программы.

2. False sharing
Когда разные потоки работают с переменными которые попали в одну строку кеша. Процессор постоянно синхронизирует кеш между ядрами, тормозя всё.
Пример: массив счётчиков где каждый поток увеличивает свой элемент, но элементы рядом в памяти.

3. Конкуренция за ресурсы
Много потоков пытаются получить доступ к одному ресурсу (mutex, файл, переменная). Образуется очередь.
Пример: все потоки пытаются добавить результат в одну общую переменную.

4. Плохая локальность данных
Обращения к памяти хаотичные, кеш не помогает. Процессор постоянно ждёт данных из медленной RAM.
Пример: обход массива не последовательно, а с большими прыжками.

5. Несбалансированная нагрузка
Одни потоки закончили работу и ждут, пока другие доделывают свою часть.
Пример: один поток получил сложные данные, остальные простые.

GPU УЗКИЕ МЕСТА:

1. Передача данных CPU ↔ GPU
Копирование через PCIe шину очень медленное по сравнению с вычислениями на GPU. Часто это главное узкое место.
Пример: копируем 1 ГБ данных на GPU за 100 мс, обрабатываем за 10 мс. 90% времени - передача!

2. Неэффективный доступ к глобальной памяти
Когда потоки в warp обращаются к несмежным адресам памяти. Каждому потоку нужна отдельная транзакция вместо одной общей.
Пример: поток 0 читает элемент 0, поток 1 читает элемент 1000 - нет коалесцированного доступа.

3. Дивергенция потоков (warp divergence)
Потоки в одном warp выполняют разные ветки кода (if-else). GPU выполняет обе ветки последовательно.
Пример: if (threadIdx.x < 16) { A } else { B } - половина потоков простаивает в каждой ветке.

4. Недостаточная загрузка GPU
Слишком мало потоков или блоков. GPU имеет тысячи ядер, но использует только часть.
Пример: запускаем 256 потоков на GPU с 4096 ядрами - 94% простаивает.

5. Bank conflicts в разделяемой памяти
Несколько потоков обращаются к одному банку разделяемой памяти одновременно. Доступы сериализуются.
Пример: потоки читают элементы с одинаковым смещением (каждый кратный 32).

6. Регистровое давление
Ядро использует слишком много регистров на поток. GPU не может запустить много потоков одновременно.
Пример: в ядре 100 локальных переменных, все в регистрах.

РАСПРЕДЕЛЁННЫЕ ПРОГРАММЫ УЗКИЕ МЕСТА:

1. Коммуникация между процессами
Отправка данных по сети медленная. Латентность и ограниченная пропускная способность.
Пример: MPI_Send занимает больше времени чем сами вычисления.

2. Синхронизация (барьеры)
Быстрые процессы ждут медленные в точках синхронизации.
Пример: MPI_Barrier - все процессы стоят и ждут самый медленный.

3. Несбалансированная нагрузка
Процессы получили разное количество работы. Одни закончили, другие ещё работают.
Пример: процессу 0 дали 1000 элементов, процессу 1 дали 5000 элементов.

4. Накладные расходы коллективных операций
MPI_Reduce, MPI_Gather, MPI_Allreduce требуют участия всех процессов. Время растёт с количеством процессов.
Пример: MPI_Allreduce с 2 процессами быстро, с 1000 процессами медленно.

5. Узкие места в сети
Ограниченная пропускная способность между узлами. Конкуренция за сетевые ресурсы.
Пример: все процессы одновременно отправляют данные главному процессу - перегрузка.

6. Сериализация данных
Упаковка/распаковка данных для отправки занимает время.
Пример: отправка структур с указателями требует специальной обработки.

7. Холодный старт
Запуск множества процессов, инициализация MPI занимает время.
Пример: запуск 100 процессов MPI может занять несколько секунд.

ОБЩИЕ ПАТТЕРНЫ:

Для всех типов параллельных программ характерны:
- Последовательные участки (закон Амдала)
- Синхронизация и ожидание
- Несбалансированная нагрузка
- Накладные расходы на параллелизм

Ключевая идея: нужно профилировать чтобы найти где именно узкое место, потому что интуиция часто ошибается.

===================================================================================

ВОПРОС 3: Почему увеличение числа потоков или процессов не всегда приводит к ускорению?

Интуитивно кажется: больше потоков = больше работы делается = быстрее. Но на практике всё сложнее.

ПРИЧИНЫ ПОЧЕМУ БОЛЬШЕ ≠ БЫСТРЕЕ:

1. Закон Амдала (последовательные части)

В программе всегда есть части которые нельзя распараллелить. Даже если параллельная часть ускорится в 1000 раз, последовательная часть останется тормозом.

Формула: Ускорение = 1 / ((1-p) + p/n)
где p - доля параллельной части, n - число потоков

Пример:
Программа: 95% параллельная, 5% последовательная
- 2 потока: ускорение 1.9x (хорошо!)
- 10 потоков: ускорение 6.9x (отлично!)
- 100 потоков: ускорение 16.8x (всё меньше прирост)
- 1000 потоков: ускорение 19.6x (почти не растёт)
Максимум: 20x из-за 5% последовательного кода!

Вывод: даже маленькая последовательная часть ограничивает максимальное ускорение.

2. Накладные расходы на создание и управление потоками

Создание потока, их синхронизация, планирование - всё это занимает время.

Пример с OpenMP:
- Создать 2 потока: 10 микросекунд
- Создать 100 потоков: 500 микросекунд
- Если задача занимает 100 микросекунд, накладные расходы больше выигрыша!

Чем больше потоков, тем больше накладные расходы.

3. Конкуренция за ресурсы

У компьютера ограниченные ресурсы:
- Количество физических ядер
- Размер кеша
- Пропускная способность памяти
- Ширина шины

Когда потоков больше чем ядер, операционная система переключается между ними (context switch) - теряется время.

Пример:
У нас 8 ядер:
- 8 потоков: каждый на своём ядре, отлично
- 16 потоков: переключение между потоками, накладные расходы
- 64 потока: постоянное переключение, большие потери

4. Конкуренция за память и кеш

Все потоки делят одну память и кеш. Чем больше потоков, тем чаще промахи мимо кеша.

Пример:
1 поток: весь кеш для него, много попаданий
16 потоков: каждому 1/16 кеша, частые промахи
64 потока: почти весь кеш заменяется, производительность падает

5. False sharing

Когда переменные разных потоков попадают в одну строку кеша (обычно 64 байта), процессор синхронизирует кеш между ядрами. Чем больше потоков, тем больше конфликтов.

Пример:
int counters[100]; // счётчики для 100 потоков
#pragma omp parallel
{
    counters[thread_id]++; // каждый поток увеличивает свой
}
Проблема: соседние счётчики в одной строке кеша - постоянная синхронизация между ядрами.

6. Синхронизация и блокировки

Чем больше потоков, тем больше времени они проводят в очередях за доступом к общим ресурсам.

Пример с mutex:
2 потока: редкие конфликты за mutex
100 потоков: постоянная очередь за mutex, потоки ждут

7. Несбалансированная нагрузка

Если работа распределена неравномерно, увеличение потоков не поможет.

Пример:
Массив из 100 элементов, 10 потоков:
- Поток 0: 10 элементов по 1 мс = 10 мс
- Поток 1: 10 элементов по 10 мс = 100 мс
Общее время = 100 мс (ждём самый медленный)

Добавим 100 потоков:
- Всё равно один поток тормозит всех
- Время не улучшилось!

8. Для GPU: ограничения аппаратуры

GPU имеет ограничения:
- Максимум потоков на блок (обычно 1024)
- Максимум блоков на SM (Streaming Multiprocessor)
- Ограниченная разделяемая память
- Ограниченное количество регистров

После определённого числа потоков GPU физически не может запустить больше одновременно.

9. Для MPI: коммуникационные затраты

Чем больше процессов, тем больше сообщений между ними. Коммуникация начинает доминировать.

Пример:
2 процесса MPI: обмен данными 1 мс, вычисления 100 мс - отлично
100 процессов MPI: обмен данными 80 мс, вычисления 2 мс - коммуникация доминирует!

10. Размер задачи

Маленькие задачи не выигрывают от большого параллелизма.

Пример:
Отсортировать 100 элементов:
- 1 поток: 1 мс
- 10 потоков: 2 мс (накладные расходы больше выигрыша!)

ПРАКТИЧЕСКИЕ НАБЛЮДЕНИЯ:

График зависимости обычно выглядит так:
1 поток: 100 секунд (базовая линия)
2 потока: 55 секунд (ускорение 1.8x - хорошо!)
4 потока: 30 секунд (ускорение 3.3x - отлично!)
8 потоков: 18 секунд (ускорение 5.5x - замедление роста)
16 потоков: 14 секунд (ускорение 7x - почти не помогает)
32 потока: 15 секунд (ускорение 6.7x - стало хуже!)

ЗОЛОТОЕ ПРАВИЛО:

Оптимальное число потоков обычно равно или немного больше числа физических ядер.
Для CPU: threads = cores или cores * 2 (если hyper-threading)
Для GPU: нужно экспериментировать, но обычно тысячи потоков
Для MPI: зависит от задачи, но редко больше нескольких сотен процессов эффективно

ВЫВОД:
Увеличение потоков помогает до определённого предела. После этого накладные расходы, конкуренция за ресурсы и последовательные части начинают доминировать. Нужно найти баланс экспериментально.

===================================================================================

ВОПРОС 4: Как законы Амдала и Густафсона применяются при анализе масштабируемости?

Это два фундаментальных закона которые объясняют пределы ускорения параллельных программ.

ЗАКОН АМДАЛА (1967)

Основная идея:
Ускорение программы ограничено её последовательной частью. Даже если параллельная часть ускорится до бесконечности, последовательная часть останется тормозом.

Формула:
S = 1 / ((1 - p) + p/n)

где:
S - ускорение
p - доля параллельной части программы (от 0 до 1)
n - число процессоров/потоков
(1-p) - доля последовательной части

Пример расчёта:
Программа выполняется 100 секунд, из них 90 секунд можно распараллелить (p=0.9), 10 секунд нельзя (последовательная часть).

С 1 процессором: 100 сек
С 2 процессорами: 10 + 90/2 = 55 сек (ускорение 1.8x)
С 4 процессорами: 10 + 90/4 = 32.5 сек (ускорение 3.1x)
С 10 процессорами: 10 + 90/10 = 19 сек (ускорение 5.3x)
С бесконечностью процессоров: 10 + 0 = 10 сек (ускорение 10x максимум!)

Видно что максимальное ускорение = 1/(1-p) = 1/0.1 = 10x, даже если добавить миллион процессоров.

Таблица ограничений:
Последовательная часть | Максимальное ускорение
5%                    | 20x
10%                   | 10x
25%                   | 4x
50%                   | 2x

Применение при анализе:
1. Измеряем время последовательной и параллельной частей
2. Вычисляем p (долю параллельной части)
3. По формуле предсказываем максимальное ускорение
4. Определяем сколько процессоров имеет смысл использовать

Пример анализа OpenMP программы:
Профилируем и видим:
- Инициализация: 2 сек (последовательно)
- Основной цикл: 98 сек (параллельно)
- Финализация: 0 сек

p = 98/100 = 0.98
Максимальное ускорение = 1/(1-0.98) = 50x

Значит нет смысла использовать больше 50 процессоров - они не дадут дополнительного ускорения.

Ограничения закона Амдала:
- Предполагает фиксированный размер задачи
- Не учитывает накладные расходы
- Пессимистичен для больших систем

ЗАКОН ГУСТАФСОНА (1988)

Основная идея:
На практике когда у нас больше процессоров, мы решаем бо́льшую задачу, а не ту же самую быстрее. Поэтому ускорение может быть выше чем предсказывает Амдал.

Формула:
S = s + p * n

где:
S - ускорение
s - доля последовательной части
p - доля параллельной части (s + p = 1)
n - число процессоров

Это S = n - (n-1)*s

Пример:
Последовательная часть занимает 5% времени на n процессорах.

По Амдалу (фиксированная задача):
n=10: ускорение = 1/(0.05 + 0.95/10) = 6.9x
n=100: ускорение = 1/(0.05 + 0.95/100) = 17.2x

По Густафсону (масштабируемая задача):
n=10: ускорение = 0.05 + 0.95*10 = 9.55x
n=100: ускорение = 0.05 + 0.95*100 = 95.05x

Густафсон намного оптимистичнее!

Почему так?
Густафсон предполагает что размер задачи растёт с числом процессоров. Последовательная часть остаётся константой, а параллельная растёт.

Пример из жизни:
Обработка изображений:
- Загрузка изображения: 1 сек (последовательно, не зависит от размера)
- Обработка пикселей: зависит от размера (параллельно)

С 1 процессором: обрабатываем 1 МП за 10 сек (1 сек загрузка + 9 сек обработка)
С 10 процессорами: можем обработать 10 МП за 10 сек (1 сек загрузка + 9 сек обработка на каждом)

Производительность выросла в 10 раз! Хотя Амдал предсказывал меньше.

КОГДА ИСПОЛЬЗОВАТЬ КАКОЙ ЗАКОН:

АМДАЛ (Strong Scaling):
Когда размер задачи фиксирован.
Примеры:
- Обработать конкретный файл как можно быстрее
- Симулировать фиксированную модель
- Отсортировать заданный массив

Вопрос: "Насколько быстрее я решу ЭТУ задачу с N процессорами?"

ГУСТАФСОН (Weak Scaling):
Когда размер задачи растёт с ресурсами.
Примеры:
- Обработать больше данных за то же время
- Симулировать большую модель
- Обработать больше запросов

Вопрос: "Сколько данных я могу обработать с N процессорами за заданное время?"

ПРАКТИЧЕСКОЕ ПРИМЕНЕНИЕ ПРИ АНАЛИЗЕ:

Шаг 1: Запустить программу с разным числом процессоров
n=1: T1 = 100 сек
n=2: T2 = 55 сек
n=4: T4 = 30 сек
n=8: T8 = 18 сек

Шаг 2: Вычислить реальное ускорение
S2 = T1/T2 = 100/55 = 1.82x
S4 = T1/T4 = 100/30 = 3.33x
S8 = T1/T8 = 100/18 = 5.56x

Шаг 3: Определить долю параллельной части по Амдалу
Из формулы S = 1/((1-p) + p/n) решаем относительно p:
p = (n*(S-1))/(S*(n-1))

Для n=8, S=5.56:
p = (8*(5.56-1))/(5.56*(8-1)) = 0.936
Значит 93.6% программы параллельно, 6.4% последовательно.

Шаг 4: Предсказать максимальное ускорение
S_max = 1/(1-0.936) = 15.6x

Значит даже с 1000 процессорами не получим больше 15.6x ускорения.

Шаг 5: Определить оптимальное число процессоров
После 16 процессоров прирост минимален - не имеет смысла добавлять больше.

ДЛЯ WEAK SCALING:

Шаг 1: Фиксируем нагрузку на процессор
Каждый процессор обрабатывает 10 млн элементов.

n=1: 10M элементов за 100 сек
n=2: 20M элементов за 105 сек (почти то же время!)
n=4: 40M элементов за 112 сек
n=8: 80M элементов за 125 сек

Шаг 2: Анализ эффективности
Идеально: время должно оставаться постоянным.
Реально: время растёт из-за коммуникационных затрат.

Шаг 3: Вычислить эффективность
E = T1/(Tn) * 100%
E2 = 100/105 = 95.2%
E4 = 100/112 = 89.3%
E8 = 100/125 = 80.0%

Видим что эффективность падает - алгоритм масштабируется не идеально.

ВЫВОДЫ:

1. Амдал показывает пределы ускорения при фиксированной задаче
2. Густафсон показывает возможности при масштабируемой задаче
3. Амдал пессимистичен, Густафсон оптимистичен
4. На практике реальность между ними
5. Оба закона важны для понимания пределов параллелизма
6. Всегда есть точка после которой добавление ресурсов неэффективно
7. Измерения и профилирование показывают где эта точка

===================================================================================

ВОПРОС 5: Какие факторы наиболее критичны для производительности гибридных приложений?

Гибридное приложение использует и CPU и GPU одновременно. У каждого свои сильные стороны, но взаимодействие между ними - ключевой фактор производительности.

КРИТИЧНЫЕ ФАКТОРЫ (по важности):

1. ПЕРЕДАЧА ДАННЫХ МЕЖДУ CPU И GPU (самое критичное!)

Почему это проблема:
CPU и GPU имеют раздельную память. Данные нужно копировать через PCIe шину, которая намного медленнее чем память GPU или CPU.

Скорости:
- Память GPU (HBM2): ~900 ГБ/с
- Память CPU (DDR4): ~100 ГБ/с
- PCIe 3.0 x16: ~16 ГБ/с (в одну сторону)
- PCIe 4.0 x16: ~32 ГБ/с

Видно что PCIe в 30-50 раз медленнее чем внутренняя память!

Пример:
Обработка 1 ГБ данных на GPU:
- Копирование на GPU: 1000/16 = 62 мс
- Обработка на GPU: 10 мс
- Копирование обратно: 62 мс
Итого: 134 мс, из них 124 мс (92%) - передача данных!

Как оптимизировать:
а) Минимизировать передачу - держать данные на GPU как можно дольше
б) Использовать pinned memory (cudaMallocHost) - ускоряет передачу в 2x
в) Асинхронная передача (cudaMemcpyAsync) - перекрывает с вычислениями
г) Разбивать данные на чанки - конвейерная обработка
д) Использовать Unified Memory если возможно

2. БАЛАНСИРОВКА НАГРУЗКИ МЕЖДУ CPU И GPU

Проблема:
Нужно правильно распределить работу. CPU хорош для последовательной логики, GPU для массового параллелизма.

Плохой пример:
90% работы на CPU, 10% на GPU - GPU простаивает
10% на CPU, 90% на GPU - но передача данных убивает выигрыш

Хороший подход:
- CPU делает подготовку данных, сложную логику
- GPU делает массовые вычисления
- Пока GPU работает, CPU готовит следующую порцию
- Оба устройства загружены одновременно

Как найти баланс:
Экспериментально! Пробуем разные соотношения:
- 50/50
- 30/70
- 20/80
Измеряем и выбираем лучшее.

Факторы влияющие на баланс:
- Скорость CPU vs GPU
- Размер данных
- Сложность вычислений
- Время передачи

3. СИНХРОНИЗАЦИЯ И ОЖИДАНИЕ

Проблема:
CPU и GPU работают асинхронно. Нужна синхронизация в определённых точках.

Плохо:
cudaMemcpy(...); // блокирует CPU
kernel<<<...>>>(...); // CPU ждёт
cudaMemcpy(...); // снова блокирует
Всё последовательно, нет параллелизма!

Хорошо:
cudaMemcpyAsync(..., stream1); // не блокирует
kernel<<<..., stream1>>>(...); // выполняется асинхронно
cudaMemcpyAsync(..., stream1); // в конвейере
// CPU делает свою работу параллельно!
cudaStreamSynchronize(stream1); // ждём только когда нужно

4. ИСПОЛЬЗОВАНИЕ CUDA STREAMS

Streams позволяют перекрывать операции:
- Копирование в stream1 + вычисления в stream2
- Обработка chunk1 пока загружается chunk2

Пример конвейера:
Без streams:
Copy chunk1 (10ms) → Compute chunk1 (20ms) → Copy back (10ms) → 
Copy chunk2 (10ms) → Compute chunk2 (20ms) → Copy back (10ms) = 80ms

Со streams:
Copy chunk1 → Compute chunk1 + Copy chunk2 → Copy back chunk1 + Compute chunk2 + Copy chunk3 → ...
Время: ~40ms (в 2 раза быстрее!)

5. ВЫБОР ПРАВИЛЬНЫХ ОПЕРАЦИЙ ДЛЯ CPU И GPU

CPU лучше для:
- Последовательной логики
- Сложных условных операторов
- Рекурсии
- Работы с файлами и I/O
- Управления и координации

GPU лучше для:
- Массовых параллельных операций
- Простых арифметических вычислений над большими данными
- Матричных операций
- Обработки изображений/видео
- Научных симуляций

Плохо: отправлять на GPU задачу с кучей if-else и рекурсии
Плохо: делать на CPU то что легко параллелится

6. НАКЛАДНЫЕ РАСХОДЫ ЗАПУСКА ЯДЕР

Каждый запуск CUDA ядра имеет накладные расходы (~10 микросекунд).

Плохо:
for (int i = 0; i < 10000; i++) {
    kernel<<<1, 1>>>(...); // 10000 запусков!
}
Накладные расходы огромны!

Хорошо:
kernel<<<10000, 1>>>(...); // один запуск, 10000 потоков

7. РАЗМЕР ДАННЫХ

Маленькие данные:
Накладные расходы на передачу больше выигрыша от GPU.
Лучше обработать на CPU.

Пример: массив из 100 элементов
Передача: 1 мкс
Обработка GPU: 2 мкс
Передача обратно: 1 мкс
На CPU: 0.5 мкс
Вывод: GPU медленнее из-за передачи!

Большие данные:
GPU показывает преимущество.
Передача 10 мс, обработка 100 мс на CPU vs 1 мс на GPU - выигрыш есть.

Правило: GPU эффективна когда данных много (мегабайты+)

8. ГРАНУЛЯРНОСТЬ РАБОТЫ

Сколько работы делает каждый поток GPU?

Слишком мало:
Накладные расходы на управление потоками > полезная работа

Слишком много:
Недостаточный параллелизм, GPU не загружена

Оптимум:
Каждый поток делает достаточно работы, но потоков достаточно для загрузки GPU.

9. УПРАВЛЕНИЕ ПАМЯТЬЮ

Типы памяти:
- Pageable memory (обычная malloc) - медленная передача
- Pinned memory (cudaMallocHost) - быстрая передача, но ограничена
- Unified Memory - автоматическое управление, но может быть медленно

Стратегия:
- Для больших редко передаваемых данных: обычная память
- Для частых передач: pinned memory
- Для прототипирования: unified memory

10. ПОРЯДОК ОПЕРАЦИЙ

Плохо:
CPU работа → передача на GPU → GPU работа → передача обратно → CPU работа
Всё последовательно!

Хорошо:
Начать передачу → пока передаётся, CPU работает → GPU работает → пока GPU работает, CPU готовит следующее
Максимальное перекрытие!

ПРАКТИЧЕСКИЙ CHECKLIST ОПТИМИЗАЦИИ:

✓ Минимизировать передачу данных CPU↔GPU
✓ Использовать pinned memory для частых передач
✓ Асинхронные копирования (cudaMemcpyAsync)
✓ CUDA streams для параллелизма
✓ Правильно балансировать нагрузку CPU/GPU (экспериментально)
✓ Держать данные на GPU как можно дольше
✓ Перекрывать копирование и вычисления
✓ Группировать маленькие запуски ядер
✓ Использовать GPU только для больших данных
✓ CPU делает логику, GPU делает массовые вычисления
✓ Профилировать чтобы найти узкие места
✓ Измерять время передачи vs время вычислений

ТИПИЧНЫЕ ОШИБКИ:

❌ Синхронное копирование когда можно асинхронное
❌ Копировать данные туда-сюда между каждой операцией
❌ Использовать GPU для маленьких данных
❌ Не использовать streams
❌ Плохая балансировка CPU/GPU
❌ Запускать много маленьких ядер вместо одного большого
❌ Не профилировать и не измерять

ЗОЛОТОЕ ПРАВИЛО:

Передача данных - враг номер 1 гибридных приложений!
Если можешь избежать передачи - избегай.
Если не можешь избежать - делай асинхронно.
Если делаешь асинхронно - перекрывай с вычислениями.

Хорошо спроектированное гибридное приложение должно иметь:
- Время передачи << Время вычислений
- Высокую загрузку и CPU и GPU одновременно
- Минимальное время простоя устройств

===================================================================================
